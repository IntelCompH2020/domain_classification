{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92b9aba3",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0f2a0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dConfigActiveLearning = { 'init_oversampling_rate': 8,\n",
    "                          'init_weak_label_ratio': 0.1,\n",
    "                              'backup_threshold': 0.2 }\n",
    "\n",
    "dConfig = { \n",
    "            #'skipCells': [''], \n",
    "            'skipCells': [ '' ], \n",
    "            'keywords': ['Deep Learning'],\n",
    "            #'threshold': 0.15, #0.3 is better but then no active learning is required\n",
    "            'query_count': 10,\n",
    "            'max_annoatations': 100,\n",
    "            'test_size': 0.2,\n",
    "            #'datasets': [],\n",
    "            #'metrics': [] \n",
    "            }\n",
    "\n",
    "def runCell(name):\n",
    "    return (len(dConfig['skipCells']) == 0) or (np.array(dConfig['skipCells']) == name).sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7952717e",
   "metadata": {},
   "source": [
    "# imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0ed235a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import collections.abc\n",
    "collections.Iterable = collections.abc.Iterable\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# import collections.abc\n",
    "# collections.Iterable = collections.abc.Iterable\n",
    "from domain_classifier.classifier import CorpusClassifier\n",
    "from domain_classifier.active_learner import ActiveLearner\n",
    "from domain_classifier.query_strategy import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#USE ENTIRE SCREEN\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e467c86",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d34b485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus():\n",
    "    df_corpus = pd.read_feather('data/given/corpus.feather')\n",
    "    df_corpus.loc[:,['text']] = (df_corpus['title'] + '. '+ df_corpus['description'])\n",
    "    df_corpus.drop(['acronym', 'title', 'description'], inplace=True, axis=1)\n",
    "    return df_corpus\n",
    "def get_true_annotations(df_dataset,keywords):\n",
    "    return  df_dataset['text'].str.contains(keywords[0]) * 1 \n",
    "def get_weak_soft_labels(df_dataset,keywords):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    #embeddings_fname = getPath('embeddings')\n",
    "    embeddings_fname = Path('data/activeLearning/embeddings/embeddings.pkl')\n",
    "    embeddings_fname.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if embeddings_fname.exists():\n",
    "        with open(embeddings_fname, \"rb\") as f_in:\n",
    "            doc_embeddings = pickle.load(f_in)\n",
    "    else:\n",
    "        n_docs = len(df_dataset['text'].to_numpy())\n",
    "        batch_size = 32\n",
    "        doc_embeddings = model.encode(df_dataset['text'].values[0:n_docs],batch_size=batch_size,show_progress_bar=True)\n",
    "        with open(embeddings_fname, 'wb') as f_Out:\n",
    "            print('5')\n",
    "            pickle.dump(doc_embeddings,f_Out)\n",
    "\n",
    "    keyword_embeddings = model.encode(keywords)\n",
    "    distances = cosine_similarity(doc_embeddings, keyword_embeddings)\n",
    "    return np.mean(distances, axis=1)\n",
    "def get_weak_labels(df_dataset,keywords,threshold):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    #embeddings_fname = getPath('embeddings')\n",
    "    embeddings_fname = Path('data/activeLearning/embeddings/embeddings.pkl')\n",
    "    embeddings_fname.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if embeddings_fname.exists():\n",
    "        with open(embeddings_fname, \"rb\") as f_in:\n",
    "            doc_embeddings = pickle.load(f_in)\n",
    "    else:\n",
    "        n_docs = len(df_dataset['text'].to_numpy())\n",
    "        batch_size = 32\n",
    "        doc_embeddings = model.encode(df_dataset['text'].values[0:n_docs],batch_size=batch_size,show_progress_bar=True)\n",
    "        with open(embeddings_fname, 'wb') as f_Out:\n",
    "            print('5')\n",
    "            pickle.dump(doc_embeddings,f_Out)\n",
    "\n",
    "    keyword_embeddings = model.encode(keywords)\n",
    "    distances = cosine_similarity(doc_embeddings, keyword_embeddings)\n",
    "    scores = np.mean(distances, axis=1)\n",
    "    return (scores>threshold)*1 \n",
    "def get_is_test(df_dataset,test_size=0.2):\n",
    "    X_train, X_test = train_test_split( df_dataset, test_size=test_size, random_state=42, stratify = df_dataset['true_annotation'].to_numpy())\n",
    "    is_test = np.ones(len(df_dataset), dtype=bool)\n",
    "    is_test[X_train.index] = False\n",
    "    return is_test\n",
    "def reduce_data(df_dataset,factor = 0.1):\n",
    "    X_train, X_test = train_test_split( df_dataset, test_size=factor, random_state=42, stratify = df_dataset['true_annotation'].to_numpy())\n",
    "    return X_test.reset_index(drop=True)\n",
    "def oversample_minority_class(df_dataset,oversampling_rate=10,col_label='labels'):\n",
    "    if oversampling_rate < 0:\n",
    "        oversampling_rate = 10**10\n",
    "    iPositiveCount = df_dataset.loc[:][col_label].sum()\n",
    "    iNegativeCount = len(df_dataset)-iPositiveCount\n",
    "    iClassCount = int(oversampling_rate * np.min([iPositiveCount,iNegativeCount]))\n",
    "    iClassCount = int(np.min([iClassCount,np.max([iPositiveCount,iNegativeCount])]))\n",
    "    oversampling_rate = iClassCount/np.min([iPositiveCount,iNegativeCount])\n",
    "    \n",
    "    condition_positive = df_dataset.loc[:][col_label]==1\n",
    "    condition_negative = df_dataset.loc[:][col_label]==0\n",
    "    df_positive = df_dataset[condition_positive]\n",
    "    df_negative = df_dataset[condition_negative]\n",
    "    \n",
    "    n_repeat = iClassCount // len(df_positive)\n",
    "    idx_positive = df_positive.loc[df_positive.index.repeat(n_repeat)].index\n",
    "    n_sample = np.mod(iClassCount,len(df_positive))\n",
    "    idx_positive = np.concatenate([idx_positive,df_positive[:n_sample].index])\n",
    "    #idx_positive = np.concatenate([idx_positive,df_positive.sample(n_sample).index])\n",
    "    \n",
    "    n_repeat = iClassCount // len(df_negative)\n",
    "    idx_negative = df_negative.loc[df_negative.index.repeat(n_repeat)].index\n",
    "    n_sample = np.mod(iClassCount,len(df_negative))\n",
    "    idx_negative = np.concatenate([idx_negative,df_negative[:n_sample].index])\n",
    "    #idx_negative = np.concatenate([idx_negative,df_negative.sample(n_sample).index])\n",
    "    \n",
    "    indices = np.hstack([idx_positive,idx_negative])\n",
    "    return [df_dataset.loc[indices].reset_index(drop=True),oversampling_rate]   #[df_dataset.loc[indices].sample(frac=1).reset_index(drop=True),oversampling_rate]\n",
    "def save_dataset(df_dataset,path):\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_dataset.to_csv(path)\n",
    "def print_positive_negative_labels(ds):\n",
    "    positive = ds.sum().to_numpy()[0]\n",
    "    negative = len(ds) - positive\n",
    "    print(f'Positive/Negative: { positive }/{ negative }')\n",
    "def read_results():\n",
    "    cProtocols = []\n",
    "    pPath = Path(f'data/activeLearning/protocols/')\n",
    "    for fileName in os.listdir(pPath):\n",
    "        try:\n",
    "            path = f'{ pPath }\\{ fileName }'\n",
    "            file = open(path)\n",
    "            dData = json.load(file)\n",
    "            cProtocols.append(dData['data'])\n",
    "        except:\n",
    "            pass\n",
    "    return cProtocols\n",
    "\n",
    "def plot_results(cProtocol=None):\n",
    "    cProtocols = []\n",
    "    if(cProtocol == None):\n",
    "        #read from file system\n",
    "        pPath = Path(f'data/activeLearning/protocols/')\n",
    "        for fileName in os.listdir(pPath):\n",
    "            try:\n",
    "                path = f'{ pPath }\\{ fileName }'\n",
    "                file = open(path)\n",
    "                dData = json.load(file)\n",
    "                cProtocols.append(dData['data'])\n",
    "            except:\n",
    "                pass\n",
    "    else:\n",
    "        cProtocols = [cProtocol]\n",
    "\n",
    "     ##############################Evaluation##############################\n",
    "    for cProtocol in cProtocols:\n",
    "        anotations = [x1['annotations'] for x1 in cProtocol]\n",
    "        #f1s = [x1['f1'] for x1 in cProtocol]\n",
    "        f1s = [float(x1['f1_score']) for x1 in cProtocol]\n",
    "        #trainIds = np.array([x1['trainId'] for x1 in cProtocol])\n",
    "        fig = plt.figure(figsize=(30,6))#figsize=(15,4)\n",
    "        #ax1 = fig.add_subplot(111)\n",
    "        plt.ylabel('F1 Score', color='blue')\n",
    "        plt.xlabel('Annotations')\n",
    "        plt.ylim([0,1.1])\n",
    "        plt.plot(anotations,f1s)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend(['F1 Score'])\n",
    "        plt.title(f'Annotations vs. F1 Score')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd30f95",
   "metadata": {},
   "source": [
    "# Short cut( can be ignored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fb49fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if runCell('Short cut'):\n",
    "#     path = Path('data/activeLearning/dataset/active_learning.csv')\n",
    "#     df_dataset = pd.read_csv(path)\n",
    "#     clf = CorpusClassifier(path2transformers=Path('data/activeLearning/models'))\n",
    "#     clf.load('pretrained_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6458fb24",
   "metadata": {},
   "source": [
    "# prepare data for active learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23f8739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if runCell('prepare data for active learning'):\n",
    "#     df_corpus = read_corpus()\n",
    "#     df_dataset = df_corpus\n",
    "#     df_dataset.insert(2,'true_annotation',get_true_annotations(df_dataset,dConfig['keywords']))    \n",
    "#     df_dataset.insert(3,'weak_soft_label',get_weak_soft_labels(df_dataset,dConfig['keywords']))\n",
    "#     df_dataset.insert(4,'is_test',get_is_test(df_dataset,dConfig['test_size']))\n",
    "#     df_dataset.insert(5,'annotation_idx',-1)\n",
    "#     save_dataset(df_dataset,'data/activeLearning/dataset/active_learning.csv')\n",
    "df_dataset = pd.read_csv('data/activeLearning/dataset/active_learning.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dfd434",
   "metadata": {},
   "source": [
    "# pu training with weak labels (uncommented hence integrated in active learning class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b833b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if runCell('pu training with weak labels'):\n",
    "#     clf = CorpusClassifier(path2transformers=Path('data/activeLearning/models'))\n",
    "#     df_train_org = pd.DataFrame({ 'id': df_dataset['id'],'text': df_dataset['text'],'labels': df_dataset['weak_label'] } )\n",
    "#     df_train = oversample_minority_class(df_train_org,max_oversampling=1)\n",
    "#     clf.train_loop(df_train,epochs=1) #trainloader\n",
    "#     clf.save('pretrained_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b54a570",
   "metadata": {},
   "source": [
    "# preparation for active learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ddd369a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tilma\\miniconda3\\envs\\al\\lib\\site-packages\\pandas\\core\\indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    }
   ],
   "source": [
    "if runCell('preparation for active learning'):\n",
    "    df_test = df_dataset[df_dataset['is_test']]\n",
    "    df_active_learning = df_dataset[df_dataset['is_test'] == False]\n",
    "    df_test = pd.DataFrame({ 'id': df_test['id'],'text': df_test['text'],'labels': df_test['true_annotation'] } )\n",
    "    #df_test = oversample_minority_class(df_test,oversampling_rate=1)\n",
    "    \n",
    "    queryStrategy = WeakSoftLabelTrustSampling()\n",
    "    clf = CorpusClassifier(path2transformers=Path('data/activeLearning/models'))\n",
    "    active_learner = ActiveLearner(clf,queryStrategy,df_active_learning[['id','text','weak_soft_label','annotation_idx']], dConfig = dConfigActiveLearning )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d1d937",
   "metadata": {},
   "source": [
    "# active learning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89d3e5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queried new indices[23808 26766 35638 54601 23671 52287 27450 54834 46686 55226] with proba [ 0.6231013   0.6032666   0.6014756   0.5839272   0.5707051  -0.16808182\n",
      " -0.17075399 -0.17469488 -0.17804113 -0.19903731]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:02<00:00,  2.32it/s]\n",
      "Eval batch: 100%|████████████████████████████████████████████████████████████████████| 168/168 [00:03<00:00, 48.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation: {'train_loss': 4.914363773074001, 'eval_loss': 0.4681639354676008, 'tp_cont': 1.9845320582389832, 'tn_cont': 165.54798328876495, 'fp_cont': 0.4520168364979327, 'fn_cont': 0.015467941761016846, 'precision_cont': 0.8144848078053638, 'recall_cont': 0.9922660291189954, 'f1_cont': 0.8946287329736152, 'accuracy_cont': 0.9972173525124381}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 14.01it/s]\n",
      "Eval batch: 100%|████████████████████████████████████████████████████████████████████| 168/168 [00:03<00:00, 48.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation: {'train_loss': 5.2516579371877015, 'eval_loss': 0.3746318514458835, 'tp_cont': 1.9883952140808105, 'tn_cont': 165.6374011039734, 'fp_cont': 0.36260068230330944, 'fn_cont': 0.011604785919189453, 'precision_cont': 0.8457671989722132, 'recall_cont': 0.9941976070399081, 'f1_cont': 0.9139954444596303, 'accuracy_cont': 0.9977725865223588}\n",
      "Current F1 score (0.9139954444596303) > then old f1 score (0.8946287329736152)=>continue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 14.15it/s]\n",
      "Eval batch: 100%|████████████████████████████████████████████████████████████████████| 168/168 [00:03<00:00, 49.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation: {'train_loss': 5.700887253973633, 'eval_loss': 0.30145555222406983, 'tp_cont': 1.9911029934883118, 'tn_cont': 165.7077178955078, 'fp_cont': 0.292284834664315, 'fn_cont': 0.008897006511688232, 'precision_cont': 0.871995098221374, 'recall_cont': 0.9955514967436581, 'f1_cont': 0.9296860678361942, 'accuracy_cont': 0.9982072509745149}\n",
      "Current F1 score (0.9296860678361942) > then old f1 score (0.9139954444596303)=>continue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 14.18it/s]\n",
      "Eval batch: 100%|████████████████████████████████████████████████████████████████████| 168/168 [00:03<00:00, 50.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation: {'train_loss': 5.904570879531093, 'eval_loss': 0.2411187607795, 'tp_cont': 1.9932582378387451, 'tn_cont': 165.76579535007477, 'fp_cont': 0.23420622292906046, 'fn_cont': 0.006741762161254883, 'precision_cont': 0.8948552369498974, 'recall_cont': 0.9966291189188742, 'f1_cont': 0.9430041370352413, 'accuracy_cont': 0.9985657858164626}\n",
      "Current F1 score (0.9430041370352413) > then old f1 score (0.9296860678361942)=>continue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 14.16it/s]\n",
      "Eval batch: 100%|████████████████████████████████████████████████████████████████████| 168/168 [00:03<00:00, 50.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation: {'train_loss': 6.2802941562840715, 'eval_loss': 0.19135062652640045, 'tp_cont': 1.9948895573616028, 'tn_cont': 165.81386590003967, 'fp_cont': 0.18613531021401286, 'fn_cont': 0.005110442638397217, 'precision_cont': 0.9146569518843534, 'recall_cont': 0.9974447786803027, 'f1_cont': 0.9542586425783924, 'accuracy_cont': 0.9988616324316982}\n",
      "Current F1 score (0.9542586425783924) > then old f1 score (0.9430041370352413)=>continue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 14.11it/s]\n",
      "Eval batch: 100%|████████████████████████████████████████████████████████████████████| 168/168 [00:03<00:00, 50.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation: {'train_loss': 6.547715876367874, 'eval_loss': 0.154168083332479, 'tp_cont': 1.9960238337516785, 'tn_cont': 165.84987890720367, 'fp_cont': 0.15011897089425474, 'fn_cont': 0.003976166248321533, 'precision_cont': 0.9300517325453787, 'recall_cont': 0.9980119168753402, 'f1_cont': 0.9628340980021473, 'accuracy_cont': 0.999082767029233}\n",
      "Current F1 score (0.9628340980021473) > then old f1 score (0.9542586425783924)=>continue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 14.18it/s]\n",
      "Eval batch: 100%|████████████████████████████████████████████████████████████████████| 168/168 [00:03<00:00, 50.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation: {'train_loss': 6.740542922518216, 'eval_loss': 0.12302752491086721, 'tp_cont': 1.9969501495361328, 'tn_cont': 165.8800666332245, 'fp_cont': 0.11993146676104516, 'fn_cont': 0.0030498504638671875, 'precision_cont': 0.9433452178720456, 'recall_cont': 0.9984750747675671, 'f1_cont': 0.9701275555891447, 'accuracy_cont': 0.9992679683415727}\n",
      "Current F1 score (0.9701275555891447) > then old f1 score (0.9628340980021473)=>continue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 14.12it/s]\n",
      "Eval batch: 100%|████████████████████████████████████████████████████████████████████| 168/168 [00:03<00:00, 49.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation: {'train_loss': 6.940957748156507, 'eval_loss': 0.09748528071213514, 'tp_cont': 1.9976726770401, 'tn_cont': 165.90487551689148, 'fp_cont': 0.09512576006818563, 'fn_cont': 0.0023273229598999023, 'precision_cont': 0.9545461433922993, 'recall_cont': 0.9988363385195506, 'f1_cont': 0.9761891320744008, 'accuracy_cont': 0.999419922129242}\n",
      "Current F1 score (0.9761891320744008) > then old f1 score (0.9701275555891447)=>continue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 14.09it/s]\n",
      "Eval batch: 100%|████████████████████████████████████████████████████████████████████| 168/168 [00:03<00:00, 49.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation: {'train_loss': 7.190737975470256, 'eval_loss': 0.07801323168678209, 'tp_cont': 1.9981963634490967, 'tn_cont': 165.9238133430481, 'fp_cont': 0.07618373824516311, 'fn_cont': 0.0018036365509033203, 'precision_cont': 0.9632739736637933, 'recall_cont': 0.9990981817240487, 'f1_cont': 0.9808590821539612, 'accuracy_cont': 0.9995357894276728}\n",
      "Current F1 score (0.9808590821539612) > then old f1 score (0.9761891320744008)=>continue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 14.18it/s]\n",
      "Eval batch: 100%|████████████████████████████████████████████████████████████████████| 168/168 [00:03<00:00, 49.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation: {'train_loss': 7.525473408109974, 'eval_loss': 0.06313866865821183, 'tp_cont': 1.9985911846160889, 'tn_cont': 165.9382756948471, 'fp_cont': 0.061725691077299416, 'fn_cont': 0.0014088153839111328, 'precision_cont': 0.970040680729028, 'recall_cont': 0.9992955923075447, 'f1_cont': 0.9844508425332129, 'accuracy_cont': 0.9996241993694025}\n",
      "Current F1 score (0.9844508425332129) > then old f1 score (0.9808590821539612)=>continue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 14.09it/s]\n",
      "Eval batch: 100%|████████████████████████████████████████████████████████████████████| 168/168 [00:03<00:00, 50.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation: {'train_loss': 7.63700733454607, 'eval_loss': 0.050032495870254934, 'tp_cont': 1.9989179372787476, 'tn_cont': 165.95105254650116, 'fp_cont': 0.04894946370040998, 'fn_cont': 0.0010820627212524414, 'precision_cont': 0.976097347085079, 'recall_cont': 0.999458968638874, 'f1_cont': 0.9876400283236652, 'accuracy_cont': 0.9997021932986726}\n",
      "Current F1 score (0.9876400283236652) > then old f1 score (0.9844508425332129)=>continue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 14.15it/s]\n",
      "Eval batch: 100%|████████████████████████████████████████████████████████████████████| 168/168 [00:03<00:00, 50.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation: {'train_loss': 7.852389930412755, 'eval_loss': 0.0396315477846656, 'tp_cont': 1.9991698265075684, 'tn_cont': 165.96120417118073, 'fp_cont': 0.038794060121290386, 'fn_cont': 0.0008301734924316406, 'precision_cont': 0.9809643044330666, 'recall_cont': 0.9995849132532844, 'f1_cont': 0.9901870757812046, 'accuracy_cont': 0.9997641414641019}\n",
      "Current F1 score (0.9901870757812046) > then old f1 score (0.9876400283236652)=>continue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 14.16it/s]\n",
      "Eval batch: 100%|████████████████████████████████████████████████████████████████████| 168/168 [00:03<00:00, 50.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation: {'train_loss': 7.7479667080478976, 'eval_loss': 0.031350530101917684, 'tp_cont': 1.999358892440796, 'tn_cont': 165.96928787231445, 'fp_cont': 0.03071233059745282, 'fn_cont': 0.0006411075592041016, 'precision_cont': 0.9848713038981592, 'recall_cont': 0.999679446219898, 'f1_cont': 0.9922201280261693, 'accuracy_cont': 0.9998133723921501}\n",
      "Current F1 score (0.9922201280261693) > then old f1 score (0.9901870757812046)=>continue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 14.13it/s]\n",
      "Eval batch: 100%|████████████████████████████████████████████████████████████████████| 168/168 [00:03<00:00, 49.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation: {'train_loss': 8.22477787223761, 'eval_loss': 0.02533219987526536, 'tp_cont': 1.9995008707046509, 'tn_cont': 165.97517502307892, 'fp_cont': 0.024825322529068217, 'fn_cont': 0.0004991292953491211, 'precision_cont': 0.9877365008598741, 'recall_cont': 0.9997504353518255, 'f1_cont': 0.9937071572699558, 'accuracy_cont': 0.9998492592156409}\n",
      "Current F1 score (0.9937071572699558) > then old f1 score (0.9922201280261693)=>continue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 14.16it/s]\n",
      "Eval batch: 100%|████████████████████████████████████████████████████████████████████| 168/168 [00:03<00:00, 49.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation: {'train_loss': 8.36656883428077, 'eval_loss': 0.020172691365587525, 'tp_cont': 1.999614953994751, 'tn_cont': 165.98021125793457, 'fp_cont': 0.019790836857282557, 'fn_cont': 0.00038504600524902344, 'precision_cont': 0.9901996731177429, 'recall_cont': 0.9998074769968756, 'f1_cont': 0.9949803816990933, 'accuracy_cont': 0.9998799054606491}\n",
      "Current F1 score (0.9949803816990933) > then old f1 score (0.9937071572699558)=>continue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 14.15it/s]\n",
      "Eval batch: 100%|████████████████████████████████████████████████████████████████████| 168/168 [00:03<00:00, 49.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation: {'train_loss': 8.55318361488753, 'eval_loss': 0.01600796873390209, 'tp_cont': 1.9997035264968872, 'tn_cont': 165.98427784442902, 'fp_cont': 0.015719167378847487, 'fn_cont': 0.00029647350311279297, 'precision_cont': 0.9922005604940315, 'recall_cont': 0.9998517632479437, 'f1_cont': 0.9960114682545409, 'accuracy_cont': 0.9999046688025784}\n",
      "Current F1 score (0.9960114682545409) > then old f1 score (0.9949803816990933)=>continue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 14.00it/s]\n",
      "Eval batch: 100%|████████████████████████████████████████████████████████████████████| 168/168 [00:03<00:00, 50.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation: {'train_loss': 6.558876961651549, 'eval_loss': 0.01229062327183783, 'tp_cont': 1.9997800588607788, 'tn_cont': 165.98793876171112, 'fp_cont': 0.012061560788424686, 'fn_cont': 0.0002199411392211914, 'precision_cont': 0.9940047165385106, 'recall_cont': 0.9998900294298894, 'f1_cont': 0.9969386872424141, 'accuracy_cont': 0.9999268958219996}\n",
      "Current F1 score ({'train_loss': 6.558876961651549, 'eval_loss': 0.01229062327183783, 'tp': 2.0, 'tn': 166.0, 'fp': 0.0, 'fn': 0.0, 'precision': 0.9999999999995, 'recall': 0.9999999999995, 'f1': 0.9999999999989999, 'accuracy': 1.0, 'tp_cont': 1.9997800588607788, 'tn_cont': 165.98793876171112, 'fp_cont': 0.012061560788424686, 'fn_cont': 0.0002199411392211914, 'precision_cont': 0.9940047165385106, 'recall_cont': 0.9998900294298894, 'f1_cont': 0.9969386872424141, 'accuracy_cont': 0.9999268958219996}) is not ( or just slighty ) better than old f1 score (0.9960114682545409)=>stop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval batch: 100%|████████████████████████████████████████████████████████████████| 12203/12203 [04:12<00:00, 48.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation against all test data{'annotations': '10', 'f1_score': '0.08241390183267334', 'classifiers': [{'weak_label_threshold': '0.27345593916666666', 'f1_score': '0.9960114682545409'}]}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dClf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9152\\1587344508.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;31m#query new samples by sample strategy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mindices_queried\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactive_learner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdConfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'query_count'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m# abort condition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\TFM\\git\\domain_classification\\src\\tests\\V7\\domain_classifier\\active_learner.py\u001b[0m in \u001b[0;36mquery\u001b[1;34m(self, num_samples)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;31m#     dClf = self._getClf('is_query_lead')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclfs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mdClf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clf'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m         \u001b[0mweak_soft_label_trust\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclfs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mdClf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weak_label_ratio'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dClf' is not defined"
     ]
    }
   ],
   "source": [
    "if runCell('active learning loop'):\n",
    "    cProtocol = []\n",
    "    annotation_count = 0\n",
    "    while True:\n",
    "        \n",
    "        #query new samples by sample strategy\n",
    "        indices_queried = active_learner.query(dConfig['query_count'])\n",
    "        \n",
    "        # abort condition\n",
    "        if (len(indices_queried) == 0) or (dConfig['max_annoatations'] <= annotation_count):\n",
    "            break\n",
    "        annotation_count += dConfig['query_count']\n",
    "          \n",
    "        # Simulate user interaction here. Replace this for real-world usage.\n",
    "        y = df_dataset.loc[indices_queried, ['true_annotation']].to_numpy().flatten()\n",
    "\n",
    "        active_learner.update(y)\n",
    "        \n",
    "        result = active_learner.eval(df_test)\n",
    "        \n",
    "        lastProtocol = active_learner.getProtocol()[-1]\n",
    "        print(f'Evaluation against all test data{lastProtocol}')  \n",
    "         \n",
    "    #save_results(cProtocol)\n",
    "    active_learner.saveProtocol()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd5bc2d",
   "metadata": {},
   "source": [
    "# plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c72195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if runCell('plot it'):\n",
    "    #RESULT HANDLING\n",
    "    plot_results() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88cd001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# protocol = read_results()[0]\n",
    "# clf_idx = 0\n",
    "# active_learning_iterration_idx = 0\n",
    "# active_learning_iterration = protocol[active_learning_iterration_idx]\n",
    "\n",
    "# clf_shorts = [ {'f1_score': clf['f1_score'],'is_query_lead': clf['is_query_lead'],'weak_label_threshold': clf['weak_label_threshold'],'weak_label_ratio': clf['weak_label_ratio'],'oversampling_rate': clf['oversampling_rate']} for clf in active_learning_iterration['classifiers']]\n",
    "# print(active_learning_iterration['f1_score'])\n",
    "# [print(clf_short) for clf_short in  clf_shorts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e120d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_idxs = active_learning_iterration['classifiers'][clf_idx]['true_idxs']\n",
    "# weak_idxs = active_learning_iterration['classifiers'][clf_idx]['weak_idxs']\n",
    "# test_idxs = active_learning_iterration['classifiers'][clf_idx]['test_idxs']\n",
    "# threshold = float(protocol[active_learning_iterration_idx]['classifiers'][clf_idx]['weak_label_threshold'])\n",
    "\n",
    "# df_true_label = pd.DataFrame(df_dataset.loc[true_idxs][['id','text','true_annotation']].to_numpy(),columns=['id','text','labels'])\n",
    "# df_true_label['labels'] = df_true_label['labels'].astype('int')\n",
    "\n",
    "# df_weak_label = pd.DataFrame(df_dataset.loc[true_idxs][['id','text','weak_soft_label']].to_numpy(),columns=['id','text','labels'])\n",
    "# df_weak_label.loc[df_weak_label[df_weak_label['labels'] <= threshold].index,['labels']] = 0\n",
    "# df_weak_label.loc[df_weak_label[df_weak_label['labels'] > threshold].index,['labels']] = 1\n",
    "# df_weak_label['labels'] = df_weak_label['labels'].astype('int')\n",
    "\n",
    "\n",
    "# df_test_true_label = pd.DataFrame(df_dataset.loc[test_idxs][['id','text','true_annotation']].to_numpy(),columns=['id','text','labels'])\n",
    "# df_test_true_label['labels'] = df_test_true_label['labels'].astype('int')\n",
    "\n",
    "# df_train_mix = pd.concat([df_true_label,df_weak_label])\n",
    "# clf = CorpusClassifier(path2transformers=Path('data/activeLearning/models'))\n",
    "# clf.train_loop(df_train_mix, df_test_true_label)\n",
    "\n",
    "# result = clf.eval(df_test)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112976ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = clf.predict_proba(df_test)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf68c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = df_dataset[df_dataset['is_test']]\n",
    "# df_active_learning = df_dataset[df_dataset['is_test'] == False]\n",
    "# df_test = pd.DataFrame({ 'id': df_test['id'],'text': df_test['text'],'labels': df_test['true_annotation'] } )\n",
    "# df_test = oversample_minority_class(df_test,max_oversampling=1)\n",
    "#result = clf.eval(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e313597",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST NO ACTIVE LEARNING? WEAK LABEL THRESHOLD\n",
    "if 1 == 2:\n",
    "    amount_weak_true_labels = [30,50,100]\n",
    "    max_train_data_size = 10000\n",
    "\n",
    "    df_test = df_dataset[df_dataset['is_test']]\n",
    "    df_test = pd.DataFrame({ 'id': df_test['id'],'text': df_test['text'],'labels': df_test['true_annotation'] } )\n",
    "\n",
    "    for amount_weak_true_label in amount_weak_true_labels:\n",
    "        print(f'##############################START {amount_weak_true_label}##############################')\n",
    "        df_train = df_dataset[df_dataset['is_test']==False]\n",
    "        df_train = pd.DataFrame({ 'id': df_train['id'],'text': df_train['text'],'labels': df_train['weak_soft_label'] } )\n",
    "\n",
    "        threshold = df_train.sort_values(by=['labels'],ascending=False)['labels'].iloc[amount_weak_true_label]\n",
    "        df_train.loc[df_train[df_train['labels'] > threshold].index,['labels']] = 1\n",
    "        df_train.loc[df_train[df_train['labels'] <= threshold].index,['labels']] = 0\n",
    "        df_train['labels'] = df_train['labels'].astype('int')\n",
    "\n",
    "        check_count = len(df_train[df_train['labels']==1])\n",
    "        print(f'check:{check_count}')\n",
    "\n",
    "        oversampling_rate = max_train_data_size// amount_weak_true_label\n",
    "        #1,10,100,919(max)\n",
    "        df_train,oversampling_rate = oversample_minority_class(df_train,oversampling_rate=oversampling_rate)\n",
    "        clf = CorpusClassifier(path2transformers=Path('data/activeLearning/models'))\n",
    "        clf.train_loop(df_train, df_test)\n",
    "\n",
    "\n",
    "\n",
    "    len(df_train)\n",
    "\n",
    "    clf = CorpusClassifier(path2transformers=Path('data/activeLearning/models'))\n",
    "    clf.train_loop(df_train, df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f3b830",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST NO ACTIVE LEARNING? OVERSAMPLING THRESHOLD\n",
    "if 1 == 2:\n",
    "    oversampling_rate = 200\n",
    "    df_test = df_dataset[df_dataset['is_test']]\n",
    "    df_test = pd.DataFrame({ 'id': df_test['id'],'text': df_test['text'],'labels': df_test['true_annotation'] } )\n",
    "    df_train = df_dataset[df_dataset['is_test']==False]\n",
    "    df_train = pd.DataFrame({ 'id': df_train['id'],'text': df_train['text'],'labels': df_train['true_annotation'] } )\n",
    "    df_train,oversampling_rate = oversample_minority_class(df_train,oversampling_rate=oversampling_rate)\n",
    "    len(df_train)\n",
    "    clf = CorpusClassifier(path2transformers=Path('data/activeLearning/models'))\n",
    "    clf.train_loop(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2978b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST NO ACTIVE LEARNING? OVERSAMPLING THRESHOLD\n",
    "def buildTestSet(df_annotated,df_dataset,threshold):\n",
    "    n_positive = len(df_dataset[df_dataset['weak_soft_label']>threshold])\n",
    "    n_negative = len(df_dataset[df_dataset['weak_soft_label']<=threshold])\n",
    "    negative_ratio = n_negative/n_positive\n",
    "    \n",
    "    df_positive = df_annotated[df_annotated['labels']==1]\n",
    "    df_negative = df_annotated[df_annotated['labels']==0]\n",
    "\n",
    "    if negative_ratio > 1:\n",
    "        n = int(len(df_positive) * negative_ratio)\n",
    "        df_negative_samples = df_negative.sample(n,replace=True)\n",
    "        df_positive_samples = df_positive\n",
    "    else:\n",
    "        n = int(len(df_negative) / negative_ratio)\n",
    "        df_positive_samples = df_negative.sample(n,replace=True)\n",
    "        df_negative_samples = df_positive\n",
    "    return pd.concat([df_positive_samples,df_negative_samples])\n",
    "    \n",
    "\n",
    "if 1 == 2:\n",
    "    #GET MAX FScore after 1. Iterration (base model)\n",
    "    df_al_test = df_dataset[df_dataset['is_test']]\n",
    "    df_al_test = pd.DataFrame(df_al_test[['id','text','true_annotation']].to_numpy(),columns=['id','text','labels'])\n",
    "    df_al_test['labels'] = df_al_test['labels'].astype('int')\n",
    "\n",
    "    df_al = df_dataset[df_dataset['is_test']==False].sort_values(by=['weak_soft_label'],ascending=False)\n",
    "    df_annotation = pd.concat([df_al[:5],df_al[-5:]])\n",
    "    threshold = np.mean([np.mean(df_annotation[df_annotation['true_annotation']==1]['weak_soft_label']),np.mean(df_annotation[df_annotation['true_annotation']==0]['weak_soft_label'])])\n",
    "    df_annotation = pd.DataFrame(df_annotation[['id','text','true_annotation']].to_numpy(),columns=['id','text','labels'])\n",
    "    df_annotation['labels'] = df_annotation['labels'].astype('int')\n",
    "    df_test = buildTestSet(df_annotation,df_al,threshold)\n",
    "\n",
    "    df_train = df_al[5:]\n",
    "    df_train = df_train[:-5]\n",
    "    df_train = pd.DataFrame(df_train[['id','text','weak_soft_label']].to_numpy(),columns=['id','text','labels'])\n",
    "    df_train.loc[df_train[df_train['labels'] > threshold].index,['labels']] = 1\n",
    "    df_train.loc[df_train[df_train['labels'] <= threshold].index,['labels']] = 0\n",
    "    df_train['labels'] = df_train['labels'].astype('int')\n",
    "\n",
    "    df_train,_ = oversample_minority_class(df_train,20)\n",
    "\n",
    "    clf = CorpusClassifier(path2transformers=Path('data/activeLearning/models'))\n",
    "    clf.train_loop(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc0be55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST NO ACTIVE LEARNING? MIX WEAK LABELS AND TRUE LABELS\n",
    "if 1 == 2:\n",
    "    df_al_test = df_dataset[df_dataset['is_test']]\n",
    "    df_al_test = pd.DataFrame(df_al_test[['id','text','true_annotation']].to_numpy(),columns=['id','text','labels'])\n",
    "    df_al_test['labels'] = df_al_test['labels'].astype('int')\n",
    "\n",
    "    df_al = df_dataset[df_dataset['is_test']==False].sort_values(by=['weak_soft_label'],ascending=False)\n",
    "    df_annotation1 = pd.concat([df_al[:5],df_al[-5:]])\n",
    "    df_annotation = df_annotation1\n",
    "    threshold = np.mean([np.mean(df_annotation[df_annotation['true_annotation']==1]['weak_soft_label']),np.mean(df_annotation[df_annotation['true_annotation']==0]['weak_soft_label'])])\n",
    "    df_annotation = pd.DataFrame(df_annotation[['id','text','true_annotation']].to_numpy(),columns=['id','text','labels'])\n",
    "    df_annotation['labels'] = df_annotation['labels'].astype('int')\n",
    "\n",
    "    df_train_true, df_test = train_test_split( df_annotation, test_size=0.5, random_state=42, stratify = df_annotation['labels'].to_numpy())\n",
    "\n",
    "    df_train_weak = df_al[5:]\n",
    "    df_train_weak = df_train_weak[:-5]\n",
    "    df_train_weak = pd.DataFrame(df_train_weak[['id','text','weak_soft_label']].to_numpy(),columns=['id','text','labels'])\n",
    "    df_train_weak.loc[df_train_weak[df_train_weak['labels'] > threshold].index,['labels']] = 1\n",
    "    df_train_weak.loc[df_train_weak[df_train_weak['labels'] <= threshold].index,['labels']] = 0\n",
    "    df_train_weak['labels'] = df_train_weak['labels'].astype('int')\n",
    "    df_train_weak,_ = oversample_minority_class(df_train_weak,-1)\n",
    "\n",
    "    df_train_true,_ = oversample_minority_class(df_train_true,-1)\n",
    "    df_test = buildTestSet(df_test,df_al,threshold)\n",
    "\n",
    "    cLog = []\n",
    "    cLog2 = []\n",
    "    n_true = len(df_train_true)\n",
    "    clf = CorpusClassifier(path2transformers=Path('data/activeLearning/models'))\n",
    "    f1_train = clf.train_loop(df_train_true, df_test)\n",
    "    result = clf.eval(df_al_test)\n",
    "    cLog.append({**{'n_true_label':str(n_true), 'repeat': str(1), 'n_weak': str(0), 'f1_train': str(f1_train), 'f1_eval': result['f1_cont']},**result})\n",
    "    cLog2.append({'n_true_label':str(n_true), 'repeat': str(1), 'n_weak': str(0), 'f1_train': str(f1_train), 'f1_eval': result['f1_cont']})\n",
    "    print(cLog[-1])\n",
    "    for n_weak in [10,100,1000,10000,100000]:\n",
    "        n_weak2 = int(np.ceil(n_weak/n_true) * n_true)\n",
    "\n",
    "        max_repeats = n_weak2 // n_true\n",
    "        repeats = [1, max_repeats]\n",
    "        if max_repeats > 2:\n",
    "\n",
    "            repeats = [1, int(np.sqrt(max_repeats)), max_repeats]\n",
    "\n",
    "        if n_weak < 100000:\n",
    "            test_size = n_weak2*1.01 / len(df_train_weak)\n",
    "            _, df_train_weak_set = train_test_split( df_train_weak, test_size=test_size, random_state=42, stratify = df_train_weak['labels'].to_numpy())\n",
    "            df_train_weak_set = df_train_weak_set[:n_weak2]\n",
    "        else:\n",
    "            df_train_weak_set = df_train_weak\n",
    "\n",
    "        for repeat in repeats:\n",
    "            df_train = pd.concat([df_train_true.loc[df_train_true.index.repeat(repeat)],df_train_weak_set])\n",
    "            clf = CorpusClassifier(path2transformers=Path('data/activeLearning/models'))\n",
    "            f1_train = clf.train_loop(df_train, df_test)\n",
    "            result = clf.eval(df_al_test)\n",
    "            cLog.append({**{'n_true_label':n_true, 'repeat': repeat, 'n_weak': n_weak2, 'f1_train': f1_train, 'f1_eval': result['f1_cont']},**result})\n",
    "            cLog2.append({'n_true_label':n_true, 'repeat': repeat, 'n_weak': n_weak2, 'f1_train': f1_train, 'f1_eval': result['f1_cont']})\n",
    "            print(cLog[-1])\n",
    "\n",
    "    import json\n",
    "    dProtocol = { 'data': cLog2 }\n",
    "    protocolPath = Path(f'data/activeLearning/trueweakmix/trueweakmix.json')\n",
    "    protocolPath.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(protocolPath,'w') as outfile:\n",
    "        json.dump(dProtocol, outfile)\n",
    "\n",
    "\n",
    "    cLog = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db957e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE BASE MODEL\n",
    "df_al_test = df_dataset[df_dataset['is_test']]\n",
    "df_al_test = pd.DataFrame(df_al_test[['id','text','true_annotation']].to_numpy(),columns=['id','text','labels'])\n",
    "df_al_test['labels'] = df_al_test['labels'].astype('int')\n",
    "\n",
    "df_al = df_dataset[df_dataset['is_test']==False].sort_values(by=['weak_soft_label'],ascending=False)\n",
    "df_annotation1 = pd.concat([df_al[:5],df_al[-5:]])\n",
    "threshold = np.mean([np.mean(df_annotation1[df_annotation1['true_annotation']==1]['weak_soft_label']),np.mean(df_annotation1[df_annotation1['true_annotation']==0]['weak_soft_label'])])\n",
    "df_annotation1 = pd.DataFrame(df_annotation1[['id','text','true_annotation']].to_numpy(),columns=['id','text','labels'])\n",
    "df_annotation1['labels'] = df_annotation1['labels'].astype('int')\n",
    "\n",
    "df_train_true1, df_test1 = train_test_split( df_annotation1, test_size=0.5, random_state=42, stratify = df_annotation1['labels'].to_numpy())\n",
    "\n",
    "df_test = df_test1\n",
    "\n",
    "df_train_weak = df_al[5:]\n",
    "df_train_weak = df_train_weak[:-5]\n",
    "df_train_weak = pd.DataFrame(df_train_weak[['id','text','weak_soft_label']].to_numpy(),columns=['id','text','labels'])\n",
    "df_train_weak.loc[df_train_weak[df_train_weak['labels'] > threshold].index,['labels']] = 1\n",
    "df_train_weak.loc[df_train_weak[df_train_weak['labels'] <= threshold].index,['labels']] = 0\n",
    "df_train_weak['labels'] = df_train_weak['labels'].astype('int')\n",
    "df_train_weak,_ = oversample_minority_class(df_train_weak,-1)\n",
    "\n",
    "df_train_true1,_ = oversample_minority_class(df_train_true1,-1)\n",
    "df_test = buildTestSet(df_test,df_al,threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea999e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TRAIN BASE MODEL WITH TRUE ANNOTATIONS\n",
    "if 1 == 2:\n",
    "    clf = CorpusClassifier(path2transformers=Path('data/activeLearning/models'))\n",
    "    clf.load('base_model.pt')\n",
    "    result = clf.eval(df_al_test)\n",
    "    print(f'Result Base Model{result}')\n",
    "    clf.train_loop(df_train_true1,df_test)\n",
    "    result = clf.eval(df_al_test)\n",
    "    print(f'Result theoretically FIRSTROUND Model{result}')\n",
    "    #clf.save('FIRSTROUND.pt')\n",
    "#QUERY\n",
    "if 1 == 2:\n",
    "    clf = CorpusClassifier(path2transformers=Path('data/activeLearning/models'))\n",
    "    clf.load('FIRSTROUND.pt')\n",
    "    result = clf.eval(df_al_test)\n",
    "    print(f'Result loaded FIRSTROUND Model{result}')\n",
    "    df_unlabeled = df_al[5:]\n",
    "    df_unlabeled = df_unlabeled[:-5]\n",
    "    df_unlabeled = pd.DataFrame(df_unlabeled[['id','text','true_annotation']].to_numpy(),columns=['id','text','labels'])\n",
    "    df_unlabeled['labels'] = df_unlabeled['labels'].astype('int')\n",
    "    predictions = clf.predict_proba(df_unlabeled)\n",
    "    query_idx = np.argsort(np.abs(predictions - 0.5))[:10]\n",
    "    clf.predict_proba(df_unlabeled.loc[query_idx])\n",
    "# #SECOND ROUND\n",
    "if 1 == 2:\n",
    "    #result = clf.eval(df_al_test)\n",
    "    #print(result)\n",
    "    df_annotation2 = df_unlabeled.loc[query_idx]\n",
    "    df_annotation = df_dataset.loc[np.concatenate([df_al[:5].index,df_al[-5:].index,df_annotation2.index])]\n",
    "    threshold2 = np.mean([np.mean(df_annotation[df_annotation['true_annotation']==1]['weak_soft_label']),np.mean(df_annotation[df_annotation['true_annotation']==0]['weak_soft_label'])])    \n",
    "\n",
    "    df_train_true2, df_test2 = train_test_split( df_annotation2, test_size=0.5, random_state=42, stratify = df_annotation1['labels'].to_numpy())\n",
    "\n",
    "    df_train_true = pd.concat([df_train_true1,df_train_true2])\n",
    "    df_train_true,_ = oversample_minority_class(df_train_true,-1)\n",
    "\n",
    "    print(len(df_train_true))\n",
    "\n",
    "    df_test = pd.concat([df_test1,df_test2])\n",
    "    df_test = buildTestSet(df_test,df_al,threshold)\n",
    "    \n",
    "    clf = CorpusClassifier(path2transformers=Path('data/activeLearning/models'))\n",
    "    clf.load('FIRSTROUND.pt')\n",
    "    clf.train_loop(df_train_true,df_test)\n",
    "    result = clf.eval(df_al_test)\n",
    "    print(f'SECONDROUND Model after annotation{result}')\n",
    "if 1 == 2:\n",
    "    df_train_weak_org = df_dataset.loc[df_train_weak_org.index]\n",
    "    df_train_weak_org.loc[df_train_weak_org[df_train_weak_org['weak_soft_label'] > threshold2].index,['labels']] = 1\n",
    "    df_train_weak_org.loc[df_train_weak_org[df_train_weak_org['weak_soft_label'] <= threshold2].index,['labels']] = 0\n",
    "    df_train_weak_org['labels'] = df_train_weak_org['labels'].astype('int')\n",
    "\n",
    "    # #&df_dataset['weak_soft_label'] < threshold2\n",
    "    df_change_train = df_train_weak_org.loc[(df_train_weak_org[df_train_weak_org['weak_soft_label'] > threshold1].index)&(df_train_weak_org[df_train_weak_org['weak_soft_label'] < threshold2].index)]\n",
    "    print(len(df_train_weak_org))\n",
    "    print(len(df_change_train))\n",
    "    \n",
    "    df0 = df_train_weak_org[df_train_weak_org['labels']==1]\n",
    "    df1 = df_train_weak_org[df_train_weak_org['labels']==1].sample(4)\n",
    "\n",
    "    df_change_train = pd.concat([df0,df1,df_change_train])\n",
    "\n",
    "#     clf = CorpusClassifier(path2transformers=Path('data/activeLearning/models'))\n",
    "#     clf.load('FIRSTROUND.pt')\n",
    "    clf.train_loop(df_change_train,df_test)\n",
    "    result = clf.eval(df_al_test)\n",
    "    print(f'SECONDROUND Model after threshold change{result}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
